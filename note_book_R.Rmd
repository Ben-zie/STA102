---
title: "Régression linéaire multiple"
output: html_notebook
---

Ce bloc-note reprend le contenu du cours STA102 du Conservatoire National des Arts et Métier (CNAM) sur les **modéles linéaires**. Il décrit les étapes d'une **régression linéaire multiple** à *P* variables explicatives. 

Destiné à un usage personnel, il comprend les scripts les plus importants pour effectuer calculs et production des différents outils (matrices, vecteurs paramètres et de variance, testes statistiques). 

Un éventuel lecteur intéressé par ce document sera invité à laisser suggestions et remarques (voir messagerie GitHub).

### Préparation : 

Le modèle peut s'écrire de la façon suivante : 
$$y_{i}=\beta_{0}+\Sigma_{j=1}^{p} \ \beta_{j} \ x_{ij} + \epsilon_{i}$$

Chargement des libraries :
```{r Chargement des bibliotheques}
library(corrplot)
```

Chargement du jeu de donnée :
```{r Chargement du jeu de donnees}
df <- read.csv2(
  "D:/Etudes/2025_2026/STA102/3.Regression multiple/TP_regression_multiple/ozone.csv"
)
```

description du jeu de données :
```{r Description du jeu de donnees}
# Caracterisatin du jeu de donnees : 
summary(df)
sum(is.na(df))
```

Création des matrices de travail :
```{r Preparation des matrices}
# Vecteur Y (variable a expliquer) :
Y = as.matrix(df[, 1])
# Matrice X (intercept + observations) :
X = cbind(matrix(
  nrow = dim(df)[1],
  ncol = 1,
  data = 1
), as.matrix(df[, -1]))
# Vecteur vide B (paramètres du modele) : 
B = matrix(nrow = dim(df)[2], ncol = 1)
# Vecteur vide Epsilon (erreurs aleatoires) :
Epsilon = matrix(nrow = dim(df)[1], ncol = 1)
```

Vérification des tailles des matrices :
```{r Banc de verification des tailles de matrices}
# Verification des dimensions des matrices : 
# print("Dimensions de Y :")
# dim(Y)
# print("Dimensions de X :")
# dim(X)
# print("Dimensions de B :")
# dim(B)
# print("Dimensions de Epsilon :")
# dim(Epsilon)
```

Affichage de la matrice de corrélation :
```{r matrice de correlation des observations}
# Affichage de la matrice de correlation de X :
corrplot(cor(X))
```

## Modèle :

Le modèle peut maintenant s'écrire de la façon suivante :

$$\mathcal{y}=X\beta +\epsilon$$

### Estimation des paramètres du modèle :

Méthode des moindres carrés :
$$Min((y-X\beta)^{T}(y-X\beta))$$

- où :
$$(y-X\beta)^{T}(y-X\beta)=(y^{T}-\beta^{T}X^{T})(y-X\beta)=y^{T}y-\beta^{T}X^{T}y-y^{T}X\beta+\beta^{T}X^{T}X\beta$$      

- L'expression comportant un **scalaire** ($\beta^{T}X^{T}y=y^{T}X\beta$), on peut écrire : 
$$y^{T}y-\beta^{T}X^{T}y-y^{T}X\beta+\beta^{T}X^{T}X\beta=y^{T}y-2\beta^{T}X^{T}y+\beta^{T}X^{T}X\beta$$
- Dérivation de $(y-X\beta)^{T}(y-X\beta)$ par $\beta$:
$$\frac{\partial(y^{T}y-2\beta^{T}X^{T}y+\beta^{T}X^{T}X\beta)}{\partial\beta}=2X^{T}y-2X^{T}X\beta=2X^{T}(y-X\beta)=2X^{T}y-2X^{T}X\beta=0$$
- Résolution :
$$X^{T}y-X^{T}X\beta=0$$
$$X^{T}X\beta=X^{T}y$$
$$\beta=(X^{T}X)^{-1}X^{T}y$$

### Estimateur des moindres carrés :
$$EMC=(X^{T}X)^{-1} \ X^{T}$$
```{r Estimateur des moindres carres}
# Estimateur des moindres carres (EMC) :
EMC <- function (Y, X, A_ret = F) {
  # Entree : vecteur Y, matrice X, option de retour de l'EMC par la fonction
  # - calcul l'EMC (A) a partir de la matrice X
  # - applique l'estimateur a la matrice d'observations X
  # - selon l'option choisie, ajoute au l'EMC au retour de la fonction
  # Sortie : liste comprenant le vecteur des estimations (Y_hat) +/- l'EMC
  A = solve(t(X) %*% X) %*% t(X)
  Y_hat = A %*% Y
  if (A_ret == T) {
    res = list(Y_hat, A)
  }else{
    res = Y_hat
  }
  return(res)
}

# Realisation de l'EMC :
B = EMC(Y = Y, X = X, A_ret = F)
# Estimation de Epsilon :
Epsilon = Y - X %*% B
```

## Thérorème de Gauss-Markov :

```{r}
# Determination de la VARIANCE de B :
variance_B <- function (y, x) {
  sigma_2 = var(y)
  V_B = as.numeric(sigma_2) * solve(t(x) %*% x)
  return(V_B)
}
# realisation de l'estimateur :
V_B = variance_B(y = Y, x = X)
```

## Matrices caractéristiques :

- Hat-matrice :
$$\hat{y}=X\beta=X(X^{T}X)^{-1}X^{T}y=Hy$$
$$H=X(X^{T}X)^{-1}X^{T}$$

- M :
$$\hat{y}=X\beta=X(X^{T}X)^{-1}X^{T}y=Hy$$
$$H=X(X^{T}X)^{-1}X^{T}$$
```{r}
matrice_Hat <- function(X) {
  H = X %*% solve(t(X) %*% X) %*% t(X)
  return(H)
}
H = matrice_Hat(X)
leviers = diag(H)
```

```{r}
matrice_M <- function(X) {
  H = matrice_Hat(X)
  M = diag(dim(H)[1]) - H
  return(M)
}

M = matrice_M(X)
# Verification de M par comparaison des residus observes (a 6 decimales) :
residus_obs = M %*% Y
if(all(round(residus_obs,6)==round(Epsilon,6))){
  print("Matrice M correcte")
}

V_Y = as.numeric(var(Y))
V_e = V_Y * H
```

```{r}
# Verification de la loi des residux observes :
# Moyenne (doit etre nulle)
mean(Epsilon)
# Estimation de la variable des residux observes : 
variance_residus <- function(X, Y, Epsilon_ret = F) {
  M = matrice_M(X)
  residus_obs = M %*% Y
  sigma_2 = sum(residus_obs ** 2) / (dim(X)[1] - dim(X)[2] - 1)
  if (Epsilon_ret == T) {
    res = list(sigma_2, residus_obs)
  } else{
    res = sigma_2
  }
  return(res)
}
sigma_2 = variance_residus(X, Y)
# Estimation de la variance de B :
variance_betas <- function(X, Y) {
  res = matrix(nrow = dim(X)[2])
  residus_obs = M %*% Y
  variance_residus = sum(residus_obs ** 2) / (dim(X)[1] - dim(X)[2] - 1)
  V_B = variance_residus * solve(t(X) %*% X)
  # Selection de la variance des estimateurs Betas :
  for (i in 1:dim(X)[2]) {
    res[i] = V_B[i, i]
  }
  return(res)
}
V_B=variance_betas(X,Y)
```

```{r}
SCT=t(Epsilon) %*% Epsilon
# SCE=t(Epsilon) %*% M %*% Epsilon
SCR=t(Epsilon) %*% H %*% Epsilon
```

```{r}
# Estimateur de projection sur le vecteur unitaire moyen :
n=dim(X)[1]
P_unitaire_moyen=matrix(nrow = n,ncol = n,1/n)
# Determination du vecteur de moyennes de Y par projection sur le vecteur unitaire moyen :
moyenne_Y=P_unitaire_moyen%*%Y
# Norme du vecteur Y :
mod_Y = t(Y)%*%Y
# Norme du vecteur Y_barre dans l'espace des observations :
mod_Y_barre = t(Y)%*%P_unitaire_moyen%*%Y
# Norme du vecteur de la somme des carres totale :
SCT=t(Y)%*%(diag(112)-P_unitaire_moyen)%*%Y
SCR=t(Y)%*%(diag(112)-H)%*%Y
SCE=mod_SCR=t(Y)%*%(H-P_unitaire_moyen)%*%Y
# Verification de la complementarite des modules de la moyenne et des carres des residus
# pour former Y :
(as.numeric(mod_Y)-as.numeric(mod_SCT+mod_Y_barre))<10**10
```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
